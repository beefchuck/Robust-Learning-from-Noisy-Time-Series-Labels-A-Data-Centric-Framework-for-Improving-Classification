{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSsKULYK9K9X"
      },
      "outputs": [],
      "source": [
        "### INTEGRATED ECG5000 BASELINE MODELS WITH TACL-NET ###\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from collections import deque, Counter\n",
        "import warnings\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# 0. ECG5000 DATA LOADER (Same as before)\n",
        "class ECG5000DataLoader:\n",
        "    \"\"\"Load and preprocess ECG5000 Dataset from local files with enhanced logging\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir=\"/content\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_labels = [\n",
        "            'Normal', 'R-on-T PVC', 'PVC', 'SP', 'UB'\n",
        "        ]\n",
        "\n",
        "    def find_ecg_files(self):\n",
        "        \"\"\"Find ECG5000 files in the directory\"\"\"\n",
        "        possible_names = [\n",
        "            'ECG5000_TRAIN.arff', 'ECG5000_TEST.arff',\n",
        "            'ECG5000_TRAIN.txt', 'ECG5000_TEST.txt',\n",
        "            'ECG5000_TRAIN.csv', 'ECG5000_TEST.csv',\n",
        "            'ECG5000.arff', 'ECG5000.txt', 'ECG5000.csv'\n",
        "        ]\n",
        "\n",
        "        found_files = {}\n",
        "        for root, dirs, files in os.walk(self.data_dir):\n",
        "            for file in files:\n",
        "                if any(name.lower() in file.lower() for name in ['ecg5000', 'ecg_5000']):\n",
        "                    if 'train' in file.lower():\n",
        "                        found_files['train'] = os.path.join(root, file)\n",
        "                    elif 'test' in file.lower():\n",
        "                        found_files['test'] = os.path.join(root, file)\n",
        "                    else:\n",
        "                        found_files['combined'] = os.path.join(root, file)\n",
        "\n",
        "        return found_files\n",
        "\n",
        "    def read_arff_file(self, filepath):\n",
        "        \"\"\"Read ARFF file format\"\"\"\n",
        "        data_lines = []\n",
        "        reading_data = False\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.startswith('@data'):\n",
        "                    reading_data = True\n",
        "                    continue\n",
        "\n",
        "                if reading_data and line and not line.startswith('%'):\n",
        "                    values = [float(x.strip()) for x in line.split(',')]\n",
        "                    data_lines.append(values)\n",
        "\n",
        "        return np.array(data_lines)\n",
        "\n",
        "    def read_csv_or_txt(self, filepath):\n",
        "        \"\"\"Read CSV or TXT file with robust parsing\"\"\"\n",
        "        print(f\"Reading file: {filepath}\")\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            first_few_lines = [f.readline().strip() for _ in range(3)]\n",
        "\n",
        "        print(f\"First few lines preview:\")\n",
        "        for i, line in enumerate(first_few_lines):\n",
        "            preview = line[:100] + \"...\" if len(line) > 100 else line\n",
        "            print(f\"Line {i+1}: {preview}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Attempting to read with pandas (space-separated)...\")\n",
        "            data = pd.read_csv(filepath, sep='\\s+', header=None)\n",
        "            print(f\"Success! Shape: {data.shape}\")\n",
        "            return data.values\n",
        "        except Exception as e:\n",
        "            print(f\"Pandas space-separated failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Attempting to read with pandas (comma-separated)...\")\n",
        "            data = pd.read_csv(filepath, sep=',', header=None)\n",
        "            print(f\"Success! Shape: {data.shape}\")\n",
        "            return data.values\n",
        "        except Exception as e:\n",
        "            print(f\"Pandas comma-separated failed: {e}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Attempting manual parsing...\")\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            data_lines = []\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#') or line.startswith('@'):\n",
        "                    continue\n",
        "\n",
        "                for delimiter in [' ', '\\t', ',', ';']:\n",
        "                    try:\n",
        "                        if delimiter == ' ':\n",
        "                            parts = [x for x in line.split() if x]\n",
        "                        else:\n",
        "                            parts = line.split(delimiter)\n",
        "\n",
        "                        if len(parts) > 1:\n",
        "                            values = []\n",
        "                            for part in parts:\n",
        "                                part = part.strip()\n",
        "                                if part:\n",
        "                                    if 'e' in part.lower() or 'E' in part:\n",
        "                                        values.append(float(part))\n",
        "                                    else:\n",
        "                                        values.append(float(part))\n",
        "\n",
        "                            if len(values) > 10:\n",
        "                                data_lines.append(values)\n",
        "                                break\n",
        "                    except (ValueError, IndexError) as e:\n",
        "                        continue\n",
        "\n",
        "                if i > 0 and i % 1000 == 0:\n",
        "                    print(f\"Processed {i} lines...\")\n",
        "\n",
        "            if len(data_lines) == 0:\n",
        "                raise ValueError(\"No valid data lines found\")\n",
        "\n",
        "            lengths = [len(row) for row in data_lines]\n",
        "            if len(set(lengths)) > 1:\n",
        "                print(f\"Warning: Inconsistent row lengths: {set(lengths)}\")\n",
        "                most_common_length = max(set(lengths), key=lengths.count)\n",
        "                data_lines = [row for row in data_lines if len(row) == most_common_length]\n",
        "                print(f\"Using rows with length {most_common_length}, kept {len(data_lines)} rows\")\n",
        "\n",
        "            result = np.array(data_lines)\n",
        "            print(f\"Manual parsing success! Shape: {result.shape}\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Manual parsing failed: {e}\")\n",
        "            raise ValueError(f\"Could not parse file {filepath}. All methods failed.\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load ECG5000 Dataset from local files with enhanced validation\"\"\"\n",
        "\n",
        "        print(\"Searching for ECG5000 dataset files...\")\n",
        "        found_files = self.find_ecg_files()\n",
        "\n",
        "        if not found_files:\n",
        "            print(\"ECG5000 dataset files not found!\")\n",
        "            print(\"Please ensure the ECG5000 dataset is in your directory.\")\n",
        "            print(\"Expected file names: ECG5000_TRAIN.arff, ECG5000_TEST.arff\")\n",
        "            print(\"Or: ECG5000.arff (combined file)\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Found files: {list(found_files.values())}\")\n",
        "\n",
        "        if 'train' in found_files and 'test' in found_files:\n",
        "            print(\"Loading separate train/test files...\")\n",
        "            train_file = found_files['train']\n",
        "            test_file = found_files['test']\n",
        "\n",
        "            if train_file.endswith('.arff'):\n",
        "                train_data = self.read_arff_file(train_file)\n",
        "                test_data = self.read_arff_file(test_file)\n",
        "            else:\n",
        "                train_data = self.read_csv_or_txt(train_file)\n",
        "                test_data = self.read_csv_or_txt(test_file)\n",
        "\n",
        "            print(f\"Loaded train data shape: {train_data.shape}\")\n",
        "            print(f\"Loaded test data shape: {test_data.shape}\")\n",
        "\n",
        "            if train_data.shape[1] != 141:\n",
        "                print(f\"Warning: Expected 141 columns (140 features + 1 label), got {train_data.shape[1]}\")\n",
        "                if train_data.shape[1] < 141:\n",
        "                    print(\"File might be missing the label column or have wrong format\")\n",
        "                    return None\n",
        "\n",
        "            y_train = train_data[:, 0].astype(int) - 1\n",
        "            X_train = train_data[:, 1:]\n",
        "            y_test = test_data[:, 0].astype(int) - 1\n",
        "            X_test = test_data[:, 1:]\n",
        "\n",
        "        elif 'combined' in found_files:\n",
        "            print(\"Loading combined file and splitting...\")\n",
        "            combined_file = found_files['combined']\n",
        "\n",
        "            if combined_file.endswith('.arff'):\n",
        "                combined_data = self.read_arff_file(combined_file)\n",
        "            else:\n",
        "                combined_data = self.read_csv_or_txt(combined_file)\n",
        "\n",
        "            print(f\"Loaded combined data shape: {combined_data.shape}\")\n",
        "\n",
        "            y_all = combined_data[:, 0].astype(int) - 1\n",
        "            X_all = combined_data[:, 1:]\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            print(\"Could not find suitable ECG5000 files\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Final data shapes:\")\n",
        "        print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "        print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "        if X_train.shape[1] != 140:\n",
        "            print(f\"Warning: Expected 140 features, got {X_train.shape[1]}\")\n",
        "\n",
        "        unique_train = np.unique(y_train)\n",
        "        unique_test = np.unique(y_test)\n",
        "        print(f\"Unique labels in train: {unique_train}\")\n",
        "        print(f\"Unique labels in test: {unique_test}\")\n",
        "\n",
        "        if len(unique_train) != 5:\n",
        "            print(f\"Warning: Expected 5 classes, got {len(unique_train)}\")\n",
        "\n",
        "        if np.any(y_train < 0) or np.any(y_test < 0):\n",
        "            print(\"Error: Found negative labels after conversion\")\n",
        "            return None\n",
        "\n",
        "        if np.any(y_train >= 5) or np.any(y_test >= 5):\n",
        "            print(\"Error: Found labels >= 5 after conversion\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Data loaded successfully:\")\n",
        "        print(f\"Features: {X_train.shape[1]}\")\n",
        "        print(f\"Classes: {self.class_labels}\")\n",
        "\n",
        "        train_dist = np.bincount(y_train.astype(int), minlength=5)\n",
        "        test_dist = np.bincount(y_test.astype(int), minlength=5)\n",
        "        print(f\"Train class distribution: {train_dist}\")\n",
        "        print(f\"Test class distribution: {test_dist}\")\n",
        "\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# 1. SAM OPTIMIZER FROM TACL-NET\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    \"\"\"Sharpness-Aware Minimization optimizer\"\"\"\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]\n",
        "\n",
        "        self.base_optimizer.step()\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(dtype=torch.float32).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    dtype=torch.float32\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n",
        "\n",
        "\n",
        "# 2. TACL-NET COMPONENTS\n",
        "class BalancedNoiseRobustLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, temp=0.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temp = temp\n",
        "        self.base_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, preds, targets, conf_scores=None):\n",
        "        probs = F.softmax(preds/self.temp, dim=1)\n",
        "        ce_loss = self.base_loss(preds, targets)\n",
        "        cons_loss = -torch.mean(torch.sum(probs * torch.log(probs + 1e-8), dim=1))\n",
        "        return self.alpha*ce_loss + (1-self.alpha)*cons_loss\n",
        "\n",
        "class EnhancedCNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1, seq_len=140, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 64, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(16)\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(128 * 16, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.conv_blocks(x)\n",
        "        x = x.flatten(1)\n",
        "        return self.projection(x)\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(latent_dim, latent_dim)\n",
        "        self.key = nn.Linear(latent_dim, latent_dim)\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "\n",
        "        weights = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(x.size(-1)), dim=-1)\n",
        "        attended = torch.matmul(weights, x)\n",
        "\n",
        "        gate = self.gate(attended)\n",
        "        return gate * attended + (1 - gate) * x\n",
        "\n",
        "\n",
        "# 3. MODEL DEFINITIONS\n",
        "class CNN1D_ECG_Enhanced(nn.Module):\n",
        "    def __init__(self, input_size, num_classes=5, num_channels=64):\n",
        "        super(CNN1D_ECG_Enhanced, self).__init__()\n",
        "\n",
        "        self.input_reshape = lambda x: x.unsqueeze(1)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, num_channels, kernel_size=7, padding=3)\n",
        "        self.bn1 = nn.BatchNorm1d(num_channels)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(num_channels, num_channels*2, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(num_channels*2)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(num_channels*2, num_channels*4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_channels*4)\n",
        "        self.pool3 = nn.MaxPool1d(2)\n",
        "\n",
        "        conv_output_size = self._get_conv_output_size(input_size)\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_size, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def _get_conv_output_size(self, input_size):\n",
        "        x = torch.zeros(1, 1, input_size)\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        return x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_reshape(x)\n",
        "\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        logits = self.fc3(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class BiLSTM_ECG_Enhanced(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=5):\n",
        "        super(BiLSTM_ECG_Enhanced, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        lstm_output_size = hidden_size * 2\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        x = x.unsqueeze(-1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        attention_weights = self.attention(lstm_out)\n",
        "        attention_weights = F.softmax(attention_weights.squeeze(-1), dim=1)\n",
        "\n",
        "        attended_output = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        logits = self.classifier(attended_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(0)\n",
        "        x = x + self.pe[:seq_len, :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerECG_Enhanced(nn.Module):\n",
        "    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2, num_classes=5):\n",
        "        super(TransformerECG_Enhanced, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = input_size\n",
        "\n",
        "        if d_model % nhead != 0:\n",
        "            d_model = nhead * (d_model // nhead)\n",
        "            self.d_model = d_model\n",
        "\n",
        "        self.input_projection = nn.Linear(1, d_model)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=input_size + 10)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(d_model // 2, d_model // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(d_model // 4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        x = x.unsqueeze(-1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        x_transposed = x.transpose(0, 1)\n",
        "        x_with_pos = self.pos_encoder(x_transposed)\n",
        "        x = x_with_pos.transpose(0, 1)\n",
        "\n",
        "        transformer_out = self.transformer(x)\n",
        "\n",
        "        pooled = torch.mean(transformer_out, dim=1)\n",
        "\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class TACL_ECG(nn.Module):\n",
        "    \"\"\"TACL-Net model for ECG classification\"\"\"\n",
        "    def __init__(self, seq_len=140, n_classes=5, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = EnhancedCNNEncoder(input_dim=1, seq_len=seq_len, latent_dim=latent_dim)\n",
        "        self.temporal_attn = TemporalAttention(latent_dim)\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(latent_dim//2, n_classes)\n",
        "\n",
        "        self.confidence = nn.Sequential(\n",
        "            nn.Linear(latent_dim//2, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.loss_fn = BalancedNoiseRobustLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z_attn = self.temporal_attn(z)\n",
        "        z_bottleneck = self.bottleneck(z_attn)\n",
        "\n",
        "        logits = self.classifier(z_bottleneck)\n",
        "        conf_scores = self.confidence(z_bottleneck).squeeze(-1) * 0.4 + 0.6\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'conf_scores': conf_scores\n",
        "        }\n",
        "\n",
        "\n",
        "# 4. ENHANCED TRAINING LOGGER\n",
        "class EnhancedTrainingLogger:\n",
        "    \"\"\"Enhanced training logger with comprehensive metrics for all models\"\"\"\n",
        "    def __init__(self, log_every=10):\n",
        "        self.log_every = log_every\n",
        "        self.epoch_logs = []\n",
        "\n",
        "    def log_epoch(self, epoch, loss, train_acc, val_acc, train_f1=None, val_f1=None,\n",
        "                  train_precision=None, val_precision=None, train_recall=None, val_recall=None, lr=None):\n",
        "        \"\"\"Log comprehensive epoch metrics\"\"\"\n",
        "        log_entry = {\n",
        "            'epoch': epoch,\n",
        "            'loss': loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_acc': val_acc,\n",
        "            'train_f1': train_f1,\n",
        "            'val_f1': val_f1,\n",
        "            'train_precision': train_precision,\n",
        "            'val_precision': val_precision,\n",
        "            'train_recall': train_recall,\n",
        "            'val_recall': val_recall,\n",
        "            'lr': lr\n",
        "        }\n",
        "        self.epoch_logs.append(log_entry)\n",
        "\n",
        "        if epoch == 1 or epoch % self.log_every == 0:\n",
        "            log_msg = f\"Epoch {epoch:3d}: Loss={loss:.4f}, TrainAcc={train_acc:.2f}%, ValAcc={val_acc:.2f}%\"\n",
        "            if train_f1 is not None:\n",
        "                log_msg += f\", TrainF1={train_f1:.3f}, ValF1={val_f1:.3f}\"\n",
        "            if train_precision is not None:\n",
        "                log_msg += f\", TrainP={train_precision:.3f}, ValP={val_precision:.3f}\"\n",
        "            if train_recall is not None:\n",
        "                log_msg += f\", TrainR={train_recall:.3f}, ValR={val_recall:.3f}\"\n",
        "            if lr is not None:\n",
        "                log_msg += f\", LR={lr:.6f}\"\n",
        "            print(log_msg)\n",
        "\n",
        "    def get_best_epoch(self):\n",
        "        \"\"\"Get the epoch with best validation accuracy\"\"\"\n",
        "        if not self.epoch_logs:\n",
        "            return None\n",
        "        best_epoch = max(self.epoch_logs, key=lambda x: x['val_acc'])\n",
        "        return best_epoch\n",
        "\n",
        "\n",
        "# 5. UTILITY FUNCTIONS\n",
        "def add_label_noise(y, noise_level, random_state=42):\n",
        "    \"\"\"Add label noise to the dataset\"\"\"\n",
        "    if noise_level == 0:\n",
        "        return y.copy(), np.zeros(len(y), dtype=bool)\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    n_samples = len(y)\n",
        "    n_noisy = int(n_samples * noise_level)\n",
        "\n",
        "    y_noisy = y.copy()\n",
        "    noisy_indices = np.random.choice(n_samples, n_noisy, replace=False)\n",
        "\n",
        "    num_classes = len(np.unique(y))\n",
        "    for idx in noisy_indices:\n",
        "        current_label = y[idx]\n",
        "        possible_labels = [i for i in range(num_classes) if i != current_label]\n",
        "        y_noisy[idx] = np.random.choice(possible_labels)\n",
        "\n",
        "    noise_mask = np.zeros(n_samples, dtype=bool)\n",
        "    noise_mask[noisy_indices] = True\n",
        "\n",
        "    return y_noisy, noise_mask\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "\n",
        "# 6. TRAINING FUNCTIONS\n",
        "def train_tacl_one_epoch(model, data_loader, optimizer, epoch, device):\n",
        "    \"\"\"Training function for TACL-Net with SAM optimizer\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch_X, batch_y in data_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        # Pseudo-labeling for high-confidence predictions after warmup\n",
        "        if epoch > 10:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(batch_X)\n",
        "                high_conf_mask = (outputs['conf_scores'] > 0.85) & \\\n",
        "                               (F.softmax(outputs['logits'], dim=1).max(1)[0] > 0.8)\n",
        "                if high_conf_mask.any():\n",
        "                    pseudo_labels = outputs['logits'].argmax(-1)\n",
        "                    batch_y[high_conf_mask] = pseudo_labels[high_conf_mask]\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = model.loss_fn(outputs['logits'], batch_y, outputs['conf_scores'])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            return loss\n",
        "\n",
        "        loss = closure()\n",
        "        optimizer.first_step(zero_grad=True)\n",
        "\n",
        "        closure()\n",
        "        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate predictions for metrics\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs['logits'], 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy, f1, precision, recall = calculate_metrics(all_targets, all_preds)\n",
        "\n",
        "    return total_loss / len(data_loader), accuracy * 100, f1, precision, recall\n",
        "\n",
        "def train_baseline_one_epoch(model, data_loader, optimizer, device):\n",
        "    \"\"\"Training function for baseline models\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch_X, batch_y in data_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_X)\n",
        "        loss = F.cross_entropy(logits, batch_y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy, f1, precision, recall = calculate_metrics(all_targets, all_preds)\n",
        "\n",
        "    return total_loss / len(data_loader), accuracy * 100, f1, precision, recall\n",
        "\n",
        "def evaluate_model(model, data_loader, device, model_type='baseline'):\n",
        "    \"\"\"Comprehensive evaluation for all models\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_conf = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            if model_type == 'tacl':\n",
        "                outputs = model(batch_X)\n",
        "                logits = outputs['logits']\n",
        "                conf_scores = outputs['conf_scores']\n",
        "            else:\n",
        "                logits = model(batch_X)\n",
        "                # Calculate confidence as max softmax probability\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "                conf_scores, _ = torch.max(probabilities, 1)\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(batch_y.cpu().numpy())\n",
        "            all_conf.extend(conf_scores.cpu().numpy())\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    accuracy, f1, precision, recall = calculate_metrics(all_targets, all_preds)\n",
        "    mean_confidence = np.mean(all_conf)\n",
        "\n",
        "    return accuracy * 100, f1, precision, recall, mean_confidence, all_preds, all_targets\n",
        "\n",
        "def train_model_enhanced(model, train_loader, val_loader, device, model_type='baseline',\n",
        "                        num_epochs=100, patience=15, verbose=True):\n",
        "    \"\"\"Enhanced training with comprehensive metrics logging\"\"\"\n",
        "\n",
        "    logger = EnhancedTrainingLogger(log_every=20)\n",
        "\n",
        "    # Setup optimizer based on model type\n",
        "    if model_type == 'tacl':\n",
        "        optimizer = SAM(model.parameters(), torch.optim.AdamW, rho=0.05, lr=1e-3, weight_decay=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer, T_max=30, eta_min=1e-6)\n",
        "    else:\n",
        "        if model_type == 'cnn' or model_type == 'lstm':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "        else:  # transformer\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=7, factor=0.5)\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        if model_type == 'tacl':\n",
        "            train_loss, train_acc, train_f1, train_precision, train_recall = train_tacl_one_epoch(\n",
        "                model, train_loader, optimizer, epoch + 1, device\n",
        "            )\n",
        "        else:\n",
        "            train_loss, train_acc, train_f1, train_precision, train_recall = train_baseline_one_epoch(\n",
        "                model, train_loader, optimizer, device\n",
        "            )\n",
        "\n",
        "        # Validation phase\n",
        "        val_acc, val_f1, val_precision, val_recall, val_conf, _, _ = evaluate_model(\n",
        "            model, val_loader, device, model_type\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        current_lr = optimizer.param_groups[0]['lr'] if model_type != 'tacl' else optimizer.base_optimizer.param_groups[0]['lr']\n",
        "\n",
        "        if model_type == 'tacl':\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler.step(train_loss)\n",
        "\n",
        "        # Log comprehensive metrics\n",
        "        if verbose:\n",
        "            logger.log_epoch(\n",
        "                epoch + 1, train_loss, train_acc, val_acc,\n",
        "                train_f1, val_f1, train_precision, val_precision,\n",
        "                train_recall, val_recall, current_lr\n",
        "            )\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            if verbose:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    best_epoch_info = logger.get_best_epoch()\n",
        "\n",
        "    return model, best_val_acc, {\n",
        "        'training_time': training_time,\n",
        "        'best_epoch': best_epoch_info,\n",
        "        'total_epochs': epoch + 1,\n",
        "        'logs': logger.epoch_logs\n",
        "    }\n",
        "\n",
        "def get_detailed_evaluation(model, test_loader, device, class_labels, model_type='baseline'):\n",
        "    \"\"\"Get detailed evaluation with classification report\"\"\"\n",
        "    test_acc, test_f1, test_precision, test_recall, test_conf, all_preds, all_targets = evaluate_model(\n",
        "        model, test_loader, device, model_type\n",
        "    )\n",
        "\n",
        "    report = classification_report(\n",
        "        all_targets, all_preds,\n",
        "        target_names=class_labels,\n",
        "        digits=4\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'test_acc': test_acc,\n",
        "        'test_f1': test_f1,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'confidence': test_conf,\n",
        "        'report': report,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets\n",
        "    }\n",
        "\n",
        "\n",
        "# 7. MAIN EXPERIMENT FUNCTION\n",
        "def run_integrated_experiments(X_train, y_train, X_test, y_test, device,\n",
        "                              class_labels, noise_level=0.0, verbose=True):\n",
        "    \"\"\"Run experiments with all four models: CNN, LSTM, Transformer, TACL-Net\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"INTEGRATED ECG5000 EXPERIMENT - NOISE LEVEL: {noise_level}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "    # Add label noise if specified\n",
        "    if noise_level > 0:\n",
        "        y_train_noisy, noise_mask = add_label_noise(y_train, noise_level)\n",
        "        if verbose:\n",
        "            print(f\"Added noise to {noise_mask.sum()}/{len(y_train)} training samples\")\n",
        "    else:\n",
        "        y_train_noisy = y_train.copy()\n",
        "        noise_mask = np.zeros(len(y_train), dtype=bool)\n",
        "\n",
        "    # Prepare data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Split training data for validation\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_scaled, y_train_noisy, test_size=0.2, random_state=42, stratify=y_train_noisy\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train_split), torch.LongTensor(y_train_split))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val_split), torch.LongTensor(y_val_split))\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(X_test_scaled), torch.LongTensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    results = {}\n",
        "    models = {}\n",
        "\n",
        "    # Model configurations\n",
        "    model_configs = [\n",
        "        ('1D-CNN', CNN1D_ECG_Enhanced(input_size, num_classes), 'cnn'),\n",
        "        ('Bi-LSTM', BiLSTM_ECG_Enhanced(input_size, num_classes=num_classes), 'lstm'),\n",
        "        ('Transformer', TransformerECG_Enhanced(input_size, num_classes=num_classes), 'transformer'),\n",
        "        ('TACL-Net', TACL_ECG(seq_len=input_size, n_classes=num_classes, latent_dim=64), 'tacl')\n",
        "    ]\n",
        "\n",
        "    for model_name, model, model_type in model_configs:\n",
        "        if verbose:\n",
        "            print(f\"\\n{'-'*70}\")\n",
        "            print(f\"Training {model_name}...\")\n",
        "            print(f\"{'-'*70}\")\n",
        "\n",
        "        # Train model\n",
        "        trained_model, val_acc, training_info = train_model_enhanced(\n",
        "            model, train_loader, val_loader, device, model_type, verbose=verbose\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_results = get_detailed_evaluation(\n",
        "            trained_model, test_loader, device, class_labels, model_type\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            'val_acc': val_acc,\n",
        "            'test_acc': test_results['test_acc'],\n",
        "            'test_f1': test_results['test_f1'],\n",
        "            'test_precision': test_results['test_precision'],\n",
        "            'test_recall': test_results['test_recall'],\n",
        "            'confidence': test_results['confidence'],\n",
        "            'report': test_results['report'],\n",
        "            'training_info': training_info,\n",
        "            'noise_level': noise_level\n",
        "        }\n",
        "        models[model_name] = trained_model\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{model_name} Results:\")\n",
        "            print(f\"  Validation Accuracy: {val_acc:.2f}%\")\n",
        "            print(f\"  Test Accuracy:      {test_results['test_acc']:.2f}%\")\n",
        "            print(f\"  Test F1-Score:      {test_results['test_f1']:.4f}\")\n",
        "            print(f\"  Test Precision:     {test_results['test_precision']:.4f}\")\n",
        "            print(f\"  Test Recall:        {test_results['test_recall']:.4f}\")\n",
        "            print(f\"  Mean Confidence:    {test_results['confidence']:.3f}\")\n",
        "            print(f\"  Training Time:      {training_info['training_time']:.1f}s\")\n",
        "            print(f\"  Total Epochs:       {training_info['total_epochs']}\")\n",
        "\n",
        "    return results, models\n",
        "\n",
        "def run_noise_robustness_integrated(X_train, y_train, X_test, y_test, device, class_labels, verbose=True):\n",
        "    \"\"\"Run noise robustness experiments with all four models\"\"\"\n",
        "\n",
        "    noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "    all_results = {}\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\"*80)\n",
        "        print(\"COMPREHENSIVE NOISE ROBUSTNESS EXPERIMENTS (ALL 4 MODELS)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    for noise_level in noise_levels:\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*20} NOISE LEVEL: {noise_level:.1f} {'='*20}\")\n",
        "\n",
        "        results, models = run_integrated_experiments(\n",
        "            X_train, y_train, X_test, y_test, device,\n",
        "            class_labels, noise_level, verbose=verbose\n",
        "        )\n",
        "\n",
        "        all_results[noise_level] = results\n",
        "\n",
        "        # Print summary for this noise level\n",
        "        if verbose:\n",
        "            print(f\"\\nSUMMARY FOR NOISE LEVEL {noise_level:.1f}:\")\n",
        "            print(\"-\" * 60)\n",
        "            for model_name, metrics in results.items():\n",
        "                print(f\"{model_name:12}: Acc={metrics['test_acc']:6.2f}%, \"\n",
        "                      f\"F1={metrics['test_f1']:.3f}, P={metrics['test_precision']:.3f}, \"\n",
        "                      f\"R={metrics['test_recall']:.3f}, Conf={metrics['confidence']:.3f}\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def print_comprehensive_comparison(all_results, verbose=True):\n",
        "    \"\"\"Print comprehensive comparison of all four models\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*120)\n",
        "        print(\"COMPREHENSIVE COMPARISON: ALL FOUR MODELS ACROSS NOISE LEVELS\")\n",
        "        print(\"=\"*120)\n",
        "\n",
        "    noise_levels = sorted(all_results.keys())\n",
        "    model_names = list(all_results[0.0].keys())\n",
        "\n",
        "    # Accuracy comparison\n",
        "    if verbose:\n",
        "        print(f\"\\nACCURACY COMPARISON:\")\n",
        "        print(\"-\" * 120)\n",
        "        print(f\"{'Model':<12} | {'Clean (0.0)':<12} | {'Noise 0.1':<12} | {'Noise 0.2':<12} | {'Noise 0.3':<12} | {'Avg Time(s)':<12} | {'Robustness':<10}\")\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "    for model_name in model_names:\n",
        "        if verbose:\n",
        "            row = f\"{model_name:<12} |\"\n",
        "            accuracies = []\n",
        "            for noise_level in noise_levels:\n",
        "                acc = all_results[noise_level][model_name]['test_acc']\n",
        "                accuracies.append(acc)\n",
        "                row += f\" {acc:>10.2f}% |\"\n",
        "\n",
        "            # Add average training time\n",
        "            avg_time = np.mean([all_results[noise][model_name]['training_info']['training_time']\n",
        "                              for noise in noise_levels])\n",
        "            row += f\" {avg_time:>10.1f}s |\"\n",
        "\n",
        "            # Add robustness score (clean vs noisy performance retention)\n",
        "            robustness = (accuracies[-1] / accuracies[0]) * 100\n",
        "            row += f\" {robustness:>8.1f}%\"\n",
        "            print(row)\n",
        "\n",
        "    # F1-Score comparison\n",
        "    if verbose:\n",
        "        print(f\"\\nF1-SCORE COMPARISON:\")\n",
        "        print(\"-\" * 120)\n",
        "        print(f\"{'Model':<12} | {'Clean (0.0)':<12} | {'Noise 0.1':<12} | {'Noise 0.2':<12} | {'Noise 0.3':<12} | {'F1 Drop':<12}\")\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "    for model_name in model_names:\n",
        "        if verbose:\n",
        "            row = f\"{model_name:<12} |\"\n",
        "            f1_scores = []\n",
        "            for noise_level in noise_levels:\n",
        "                f1 = all_results[noise_level][model_name]['test_f1']\n",
        "                f1_scores.append(f1)\n",
        "                row += f\" {f1:>10.4f} |\"\n",
        "\n",
        "            # Add F1 drop\n",
        "            f1_drop = f1_scores[0] - f1_scores[-1]\n",
        "            row += f\" {f1_drop:>10.4f}\"\n",
        "            print(row)\n",
        "\n",
        "    # Model rankings\n",
        "    if verbose:\n",
        "        print(f\"\\nMODEL RANKINGS:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Rank by clean performance\n",
        "        clean_ranking = sorted(model_names, key=lambda x: all_results[0.0][x]['test_acc'], reverse=True)\n",
        "        print(f\"Clean Performance:  {' > '.join(clean_ranking)}\")\n",
        "\n",
        "        # Rank by robustness\n",
        "        robustness_scores = {}\n",
        "        for model_name in model_names:\n",
        "            clean_acc = all_results[0.0][model_name]['test_acc']\n",
        "            noisy_acc = all_results[0.3][model_name]['test_acc']\n",
        "            robustness_scores[model_name] = (noisy_acc / clean_acc) * 100\n",
        "\n",
        "        robustness_ranking = sorted(model_names, key=lambda x: robustness_scores[x], reverse=True)\n",
        "        print(f\"Noise Robustness:   {' > '.join(robustness_ranking)}\")\n",
        "\n",
        "        # Rank by training efficiency\n",
        "        efficiency_scores = {}\n",
        "        for model_name in model_names:\n",
        "            avg_time = np.mean([all_results[noise][model_name]['training_info']['training_time']\n",
        "                              for noise in noise_levels])\n",
        "            efficiency_scores[model_name] = avg_time\n",
        "\n",
        "        efficiency_ranking = sorted(model_names, key=lambda x: efficiency_scores[x])\n",
        "        print(f\"Training Efficiency: {' > '.join(efficiency_ranking)} (fastest to slowest)\")\n",
        "\n",
        "def generate_model_recommendations(all_results, verbose=True):\n",
        "    \"\"\"Generate comprehensive model recommendations - DISABLED\"\"\"\n",
        "    # This function now does nothing - recommendations removed as requested\n",
        "    pass\n",
        "\n",
        "def create_comprehensive_plots(all_results, save_prefix=\"ecg_four_models\", verbose=True):\n",
        "    \"\"\"Create comprehensive visualization plots for all four models\"\"\"\n",
        "    try:\n",
        "        noise_levels = sorted(all_results.keys())\n",
        "        model_names = list(all_results[0.0].keys())\n",
        "\n",
        "        # Set up the plotting style\n",
        "        plt.style.use('default')\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
        "\n",
        "        # Create comprehensive figure with 11 subplots (removed subplot 12)\n",
        "        fig = plt.figure(figsize=(24, 16))\n",
        "\n",
        "        # 1. Accuracy vs Noise Level\n",
        "        plt.subplot(3, 4, 1)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            accuracies = [all_results[noise][model_name]['test_acc'] for noise in noise_levels]\n",
        "            plt.plot(noise_levels, accuracies, marker='o', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "        plt.title('Test Accuracy vs Label Noise', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. F1-Score vs Noise Level\n",
        "        plt.subplot(3, 4, 2)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            f1_scores = [all_results[noise][model_name]['test_f1'] for noise in noise_levels]\n",
        "            plt.plot(noise_levels, f1_scores, marker='s', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Test F1-Score', fontsize=12)\n",
        "        plt.title('F1-Score vs Label Noise', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Precision vs Noise Level\n",
        "        plt.subplot(3, 4, 3)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            precisions = [all_results[noise][model_name]['test_precision'] for noise in noise_levels]\n",
        "            plt.plot(noise_levels, precisions, marker='^', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Test Precision', fontsize=12)\n",
        "        plt.title('Precision vs Label Noise', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Recall vs Noise Level\n",
        "        plt.subplot(3, 4, 4)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            recalls = [all_results[noise][model_name]['test_recall'] for noise in noise_levels]\n",
        "            plt.plot(noise_levels, recalls, marker='v', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Test Recall', fontsize=12)\n",
        "        plt.title('Recall vs Label Noise', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. Confidence vs Noise Level\n",
        "        plt.subplot(3, 4, 5)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            confidences = [all_results[noise][model_name]['confidence'] for noise in noise_levels]\n",
        "            plt.plot(noise_levels, confidences, marker='d', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Mean Confidence', fontsize=12)\n",
        "        plt.title('Model Confidence vs Label Noise', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Training Time Comparison\n",
        "        plt.subplot(3, 4, 6)\n",
        "        avg_times = []\n",
        "        for model_name in model_names:\n",
        "            times = [all_results[noise][model_name]['training_info']['training_time']\n",
        "                    for noise in noise_levels]\n",
        "            avg_time = np.mean(times)\n",
        "            avg_times.append(avg_time)\n",
        "\n",
        "        bars = plt.bar(model_names, avg_times, alpha=0.7, color=colors)\n",
        "        plt.ylabel('Average Training Time (s)', fontsize=12)\n",
        "        plt.title('Average Training Time Comparison', fontsize=14, fontweight='bold')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        for bar, time_val in zip(bars, avg_times):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01,\n",
        "                    f'{time_val:.1f}s', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # 7. Clean Dataset Performance\n",
        "        plt.subplot(3, 4, 7)\n",
        "        clean_accs = [all_results[0.0][model]['test_acc'] for model in model_names]\n",
        "        clean_f1s = [all_results[0.0][model]['test_f1'] for model in model_names]\n",
        "\n",
        "        x_pos = np.arange(len(model_names))\n",
        "        width = 0.35\n",
        "\n",
        "        bars1 = plt.bar(x_pos - width/2, clean_accs, width, label='Accuracy (%)', alpha=0.7)\n",
        "        bars2 = plt.bar(x_pos + width/2, [f1*100 for f1 in clean_f1s], width, label='F1-Score (x100)', alpha=0.7)\n",
        "\n",
        "        plt.xlabel('Model', fontsize=12)\n",
        "        plt.ylabel('Performance', fontsize=12)\n",
        "        plt.title('Clean Dataset Performance', fontsize=14, fontweight='bold')\n",
        "        plt.xticks(x_pos, model_names, rotation=45)\n",
        "        plt.legend()\n",
        "\n",
        "        # 8. Robustness Scores\n",
        "        plt.subplot(3, 4, 8)\n",
        "        robustness_scores = []\n",
        "        for model_name in model_names:\n",
        "            clean_acc = all_results[0.0][model_name]['test_acc']\n",
        "            noisy_acc = all_results[0.3][model_name]['test_acc']\n",
        "            robustness = (noisy_acc / clean_acc) * 100\n",
        "            robustness_scores.append(robustness)\n",
        "\n",
        "        bars = plt.bar(model_names, robustness_scores, alpha=0.7, color=colors)\n",
        "        plt.ylabel('Robustness Score (%)', fontsize=12)\n",
        "        plt.title('Robustness to High Noise (30%)', fontsize=14, fontweight='bold')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        for bar, score in zip(bars, robustness_scores):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                    f'{score:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # 9. Accuracy Heatmap\n",
        "        plt.subplot(3, 4, 9)\n",
        "        accuracy_matrix = []\n",
        "        for model_name in model_names:\n",
        "            model_accuracies = [all_results[noise][model_name]['test_acc'] for noise in noise_levels]\n",
        "            accuracy_matrix.append(model_accuracies)\n",
        "\n",
        "        sns.heatmap(accuracy_matrix, xticklabels=[f'{n:.1f}' for n in noise_levels],\n",
        "                   yticklabels=model_names, annot=True, fmt='.1f', cmap='RdYlBu_r', cbar_kws={'label': 'Accuracy (%)'})\n",
        "        plt.title('Accuracy Heatmap', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "\n",
        "        # 10. F1-Score Heatmap\n",
        "        plt.subplot(3, 4, 10)\n",
        "        f1_matrix = []\n",
        "        for model_name in model_names:\n",
        "            model_f1s = [all_results[noise][model_name]['test_f1'] for noise in noise_levels]\n",
        "            f1_matrix.append(model_f1s)\n",
        "\n",
        "        sns.heatmap(f1_matrix, xticklabels=[f'{n:.1f}' for n in noise_levels],\n",
        "                   yticklabels=model_names, annot=True, fmt='.3f', cmap='RdYlBu_r', cbar_kws={'label': 'F1-Score'})\n",
        "        plt.title('F1-Score Heatmap', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "\n",
        "        # 11. Epochs to Convergence\n",
        "        plt.subplot(3, 4, 11)\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            epochs = [all_results[noise][model_name]['training_info']['total_epochs']\n",
        "                     for noise in noise_levels]\n",
        "            plt.plot(noise_levels, epochs, marker='o', label=model_name,\n",
        "                    linewidth=2.5, markersize=8, color=colors[i])\n",
        "\n",
        "        plt.xlabel('Noise Level', fontsize=12)\n",
        "        plt.ylabel('Epochs to Convergence', fontsize=12)\n",
        "        plt.title('Training Convergence', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot 12 removed (Model Performance Summary)\n",
        "\n",
        "        plt.tight_layout(pad=3.0)\n",
        "\n",
        "        # Save the plot\n",
        "        if save_prefix:\n",
        "            save_path = f\"{save_prefix}_comprehensive_analysis.png\"\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            if verbose:\n",
        "                print(f\"Comprehensive plot saved to: {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Create additional training curves plot\n",
        "        create_training_curves_plot(all_results, f\"{save_prefix}_training_curves.png\", verbose)\n",
        "\n",
        "    except ImportError:\n",
        "        if verbose:\n",
        "            print(\"Matplotlib/Seaborn not available. Skipping comprehensive plots.\")\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error creating comprehensive plots: {e}\")\n",
        "\n",
        "def create_training_curves_plot(all_results, save_path=None, verbose=True):\n",
        "    \"\"\"Create detailed training curves for each model and noise level\"\"\"\n",
        "    try:\n",
        "        noise_levels = sorted(all_results.keys())\n",
        "        model_names = list(all_results[0.0].keys())\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "        fig, axes = plt.subplots(len(model_names), len(noise_levels),\n",
        "                                figsize=(20, 16), sharey=True)\n",
        "\n",
        "        if len(model_names) == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            for j, noise_level in enumerate(noise_levels):\n",
        "                ax = axes[i, j]\n",
        "\n",
        "                training_logs = all_results[noise_level][model_name]['training_info']['logs']\n",
        "\n",
        "                epochs = [log['epoch'] for log in training_logs]\n",
        "                train_accs = [log['train_acc'] for log in training_logs]\n",
        "                val_accs = [log['val_acc'] for log in training_logs]\n",
        "                losses = [log['loss'] for log in training_logs]\n",
        "                train_f1s = [log['train_f1'] if log['train_f1'] else 0 for log in training_logs]\n",
        "                val_f1s = [log['val_f1'] if log['val_f1'] else 0 for log in training_logs]\n",
        "\n",
        "                # Plot accuracy and F1 curves\n",
        "                ax2 = ax.twinx()\n",
        "                line1 = ax.plot(epochs, train_accs, 'b-', label='Train Acc', linewidth=2, alpha=0.7)\n",
        "                line2 = ax.plot(epochs, val_accs, 'g-', label='Val Acc', linewidth=2)\n",
        "                line3 = ax.plot(epochs, [f1*100 for f1 in train_f1s], 'b--', label='Train F1 (x100)', linewidth=1.5, alpha=0.7)\n",
        "                line4 = ax.plot(epochs, [f1*100 for f1 in val_f1s], 'g--', label='Val F1 (x100)', linewidth=1.5)\n",
        "                line5 = ax2.plot(epochs, losses, 'r:', label='Loss', linewidth=2, alpha=0.8)\n",
        "\n",
        "                ax.set_xlabel('Epoch', fontsize=10)\n",
        "                ax.set_ylabel('Accuracy (%) / F1 (x100)', color='black', fontsize=10)\n",
        "                ax2.set_ylabel('Loss', color='red', fontsize=10)\n",
        "                ax.set_title(f'{model_name} - Noise {noise_level:.1f}', fontsize=11)\n",
        "\n",
        "                # Mark best epoch\n",
        "                best_epoch_info = all_results[noise_level][model_name]['training_info']['best_epoch']\n",
        "                if best_epoch_info:\n",
        "                    best_epoch = best_epoch_info['epoch']\n",
        "                    best_val_acc = best_epoch_info['val_acc']\n",
        "                    ax.axvline(x=best_epoch, color='orange', linestyle=':', alpha=0.7, linewidth=2)\n",
        "                    ax.plot(best_epoch, best_val_acc, 'o', color='orange', markersize=8)\n",
        "\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                # Combine legends\n",
        "                lines = line1 + line2 + line3 + line4 + line5\n",
        "                labels = [l.get_label() for l in lines]\n",
        "                if i == 0 and j == 0:  # Only show legend on first subplot\n",
        "                    ax.legend(lines, labels, loc='lower right', fontsize=8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            if verbose:\n",
        "                print(f\"Training curves plot saved to: {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error creating training curves plot: {e}\")\n",
        "\n",
        "def create_synthetic_ecg_data():\n",
        "    \"\"\"Create synthetic ECG5000-like data for testing\"\"\"\n",
        "    print(\"Creating synthetic ECG5000 data for testing...\")\n",
        "\n",
        "    np.random.seed(42)\n",
        "    n_train, n_test = 4000, 1000\n",
        "    n_features = 140\n",
        "    n_classes = 5\n",
        "\n",
        "    # Create synthetic training data\n",
        "    X_train = np.random.randn(n_train, n_features)\n",
        "    y_train = np.random.randint(0, n_classes, n_train)\n",
        "\n",
        "    # Create synthetic test data\n",
        "    X_test = np.random.randn(n_test, n_features)\n",
        "    y_test = np.random.randint(0, n_classes, n_test)\n",
        "\n",
        "    # Add class-specific ECG-like patterns\n",
        "    for class_idx in range(n_classes):\n",
        "        train_mask = y_train == class_idx\n",
        "        test_mask = y_test == class_idx\n",
        "\n",
        "        if class_idx == 0:  # Normal\n",
        "            pattern = np.sin(np.linspace(0, 4*np.pi, n_features)) * 0.5\n",
        "        elif class_idx == 1:  # R-on-T PVC\n",
        "            pattern = np.zeros(n_features)\n",
        "            pattern[70:75] = 2.0\n",
        "        elif class_idx == 2:  # PVC\n",
        "            pattern = np.zeros(n_features)\n",
        "            pattern[60:80] = 1.0\n",
        "        elif class_idx == 3:  # SP\n",
        "            pattern = np.zeros(n_features)\n",
        "            pattern[30:40] = 1.5\n",
        "        else:  # UB\n",
        "            pattern = np.random.randn(n_features) * 0.3\n",
        "\n",
        "        X_train[train_mask] += pattern\n",
        "        X_test[test_mask] += pattern\n",
        "\n",
        "    print(f\"Generated synthetic ECG data:\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "    print(f\"Classes: {np.unique(y_train)}\")\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def save_comprehensive_results(all_results, models=None, save_dir=\"./ecg_four_models_results\", verbose=True):\n",
        "    \"\"\"Save comprehensive results including all metrics and models\"\"\"\n",
        "    try:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save all results\n",
        "        with open(os.path.join(save_dir, \"comprehensive_results.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(all_results, f)\n",
        "\n",
        "        # Save summary as JSON\n",
        "        summary_data = {}\n",
        "        for noise_level, noise_results in all_results.items():\n",
        "            summary_data[f\"noise_{noise_level}\"] = {}\n",
        "            for model_name, metrics in noise_results.items():\n",
        "                summary_data[f\"noise_{noise_level}\"][model_name] = {\n",
        "                    'test_accuracy': metrics['test_acc'],\n",
        "                    'test_f1': metrics['test_f1'],\n",
        "                    'test_precision': metrics['test_precision'],\n",
        "                    'test_recall': metrics['test_recall'],\n",
        "                    'validation_accuracy': metrics['val_acc'],\n",
        "                    'confidence': metrics['confidence'],\n",
        "                    'training_time': metrics['training_info']['training_time'],\n",
        "                    'total_epochs': metrics['training_info']['total_epochs'],\n",
        "                    'best_epoch': metrics['training_info']['best_epoch']['epoch'] if metrics['training_info']['best_epoch'] else None\n",
        "                }\n",
        "\n",
        "        with open(os.path.join(save_dir, \"results_summary.json\"), \"w\") as f:\n",
        "            json.dump(summary_data, f, indent=2)\n",
        "\n",
        "        # Save models from clean dataset\n",
        "        if models and 0.0 in all_results:\n",
        "            for model_name, model in models.items():\n",
        "                model_path = os.path.join(save_dir, f\"{model_name.lower().replace('-', '_')}_model.pth\")\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                if verbose:\n",
        "                    print(f\"Saved {model_name} model to: {model_path}\")\n",
        "\n",
        "        # Save detailed training logs as CSV\n",
        "        for noise_level, noise_results in all_results.items():\n",
        "            for model_name, metrics in noise_results.items():\n",
        "                training_logs = metrics['training_info']['logs']\n",
        "                df_logs = pd.DataFrame(training_logs)\n",
        "                log_path = os.path.join(save_dir, f\"training_logs_{model_name.lower().replace('-', '_')}_noise_{noise_level}.csv\")\n",
        "                df_logs.to_csv(log_path, index=False)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"All comprehensive results saved to: {save_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"Error saving results: {e}\")\n",
        "\n",
        "def print_classification_reports(all_results, noise_level=0.0, verbose=True):\n",
        "    \"\"\"Print detailed classification reports for all models\"\"\"\n",
        "    if not verbose:\n",
        "        return\n",
        "\n",
        "    print(f\"\\nDETAILED CLASSIFICATION REPORTS (Noise Level: {noise_level})\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results = all_results[noise_level]\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"\\n{model_name} Classification Report:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(metrics['report'])\n",
        "\n",
        "\n",
        "# 8. MAIN EXECUTION FUNCTIONS\n",
        "def main_integrated_experiment():\n",
        "    \"\"\"Main function for comprehensive four-model experiment\"\"\"\n",
        "\n",
        "    print(\"ECG5000 Integrated Baseline Models + TACL-Net Experiment\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Initialize data loader\n",
        "    data_loader = ECG5000DataLoader(data_dir=\"/content\")\n",
        "\n",
        "    # Load data\n",
        "    data = data_loader.load_data()\n",
        "\n",
        "    if data is None:\n",
        "        print(\"Failed to load ECG5000 dataset. Using synthetic data for demonstration.\")\n",
        "        X_train, y_train, X_test, y_test = create_synthetic_ecg_data()\n",
        "    else:\n",
        "        X_train, y_train, X_test, y_test = data\n",
        "\n",
        "    print(f\"\\nECG5000 Dataset Summary:\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"Test samples: {X_test.shape[0]}\")\n",
        "    print(f\"Features: {X_train.shape[1]}\")\n",
        "    print(f\"Classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "    class_labels = data_loader.class_labels\n",
        "\n",
        "    print(\"\\nExperiment Options:\")\n",
        "    print(\"1. Clean data only (all 4 models)\")\n",
        "    print(\"2. Noise robustness analysis (all noise levels)\")\n",
        "    print(\"3. Both clean and noise analysis\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Choose experiment type (1, 2, or 3, default=3): \").strip()\n",
        "    except:\n",
        "        choice = '3'  # Default for non-interactive environments\n",
        "\n",
        "    if choice == '1':\n",
        "        # Clean data only\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"CLEAN ECG DATA EXPERIMENT (ALL 4 MODELS)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        results, models = run_integrated_experiments(\n",
        "            X_train, y_train, X_test, y_test, device, class_labels,\n",
        "            noise_level=0.0, verbose=True\n",
        "        )\n",
        "\n",
        "        print_classification_reports({0.0: results}, 0.0)\n",
        "\n",
        "        return {0.0: results}, models\n",
        "\n",
        "    elif choice == '2':\n",
        "        # Noise robustness only\n",
        "        all_results = run_noise_robustness_integrated(\n",
        "            X_train, y_train, X_test, y_test, device, class_labels\n",
        "        )\n",
        "\n",
        "        print_comprehensive_comparison(all_results)\n",
        "        create_comprehensive_plots(all_results)\n",
        "\n",
        "        return all_results, None\n",
        "\n",
        "    else:\n",
        "        # Both clean and noise analysis\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE FOUR-MODEL ECG5000 ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Run noise robustness analysis\n",
        "        all_results = run_noise_robustness_integrated(\n",
        "            X_train, y_train, X_test, y_test, device, class_labels\n",
        "        )\n",
        "\n",
        "        # Print classification reports for clean data\n",
        "        print_classification_reports(all_results, 0.0)\n",
        "\n",
        "        # Comprehensive analysis\n",
        "        print_comprehensive_comparison(all_results)\n",
        "\n",
        "        # Create visualizations\n",
        "        try:\n",
        "            plot_choice = input(\"\\nGenerate comprehensive plots? (y/n, default=y): \").lower()\n",
        "        except:\n",
        "            plot_choice = 'y'\n",
        "\n",
        "        if plot_choice != 'n':\n",
        "            create_comprehensive_plots(all_results)\n",
        "\n",
        "        # Save results\n",
        "        try:\n",
        "            save_choice = input(\"Save comprehensive results and models? (y/n, default=y): \").lower()\n",
        "        except:\n",
        "            save_choice = 'y'\n",
        "\n",
        "        if save_choice != 'n':\n",
        "            clean_models = None\n",
        "            if 0.0 in all_results:\n",
        "                # Re-run clean experiment to get models\n",
        "                _, clean_models = run_integrated_experiments(\n",
        "                    X_train, y_train, X_test, y_test, device, class_labels,\n",
        "                    noise_level=0.0, verbose=False\n",
        "                )\n",
        "            save_comprehensive_results(all_results, clean_models)\n",
        "\n",
        "        return all_results, clean_models\n",
        "\n",
        "def quick_demo():\n",
        "    \"\"\"Quick demonstration of the integrated framework\"\"\"\n",
        "    print(\"ECG5000 Integrated Models (CNN + LSTM + Transformer + TACL-Net) - Quick Demo\")\n",
        "    print(\"=\"*75)\n",
        "\n",
        "    # Create synthetic demo data\n",
        "    X_train, y_train, X_test, y_test = create_synthetic_ecg_data()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    class_labels = ['Normal', 'R-on-T PVC', 'PVC', 'SP', 'UB']\n",
        "\n",
        "    print(\"\\nRunning quick integrated experiment...\")\n",
        "\n",
        "    # Run a single experiment with clean data\n",
        "    results, models = run_integrated_experiments(\n",
        "        X_train, y_train, X_test, y_test, device, class_labels,\n",
        "        noise_level=0.0, verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nQuick Demo Results Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"{model_name:12}: Acc={metrics['test_acc']:6.2f}%, \"\n",
        "              f\"F1={metrics['test_f1']:.4f}, Time={metrics['training_info']['training_time']:5.1f}s\")\n",
        "\n",
        "    print(\"\\nThis integrated framework provides:\")\n",
        "    print(\" All 4 models: 1D-CNN, Bi-LSTM, Transformer, TACL-Net\")\n",
        "    print(\" Comprehensive metrics: Accuracy, F1, Precision, Recall, Confidence\")\n",
        "    print(\" Enhanced logging with epoch-by-epoch tracking\")\n",
        "    print(\" Noise robustness analysis across multiple noise levels\")\n",
        "    print(\" Detailed visualizations and comparison plots\")\n",
        "    print(\" Complete result saving and model persistence\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ECG5000 Integrated Baseline Models + TACL-Net\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. Run full integrated experiment\")\n",
        "    print(\"2. Show quick demo\")\n",
        "    print(\"3. Run with synthetic data only\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"Choose option (1, 2, or 3, default=1): \").strip()\n",
        "\n",
        "        if choice == '2':\n",
        "            quick_demo()\n",
        "        elif choice == '3':\n",
        "            # Run with synthetic data\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"RUNNING WITH SYNTHETIC ECG5000 DATA\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            X_train, y_train, X_test, y_test = create_synthetic_ecg_data()\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            class_labels = ['Normal', 'R-on-T PVC', 'PVC', 'SP', 'UB']\n",
        "\n",
        "            # Run noise robustness analysis\n",
        "            all_results = run_noise_robustness_integrated(\n",
        "                X_train, y_train, X_test, y_test, device, class_labels\n",
        "            )\n",
        "\n",
        "            print_comprehensive_comparison(all_results)\n",
        "            create_comprehensive_plots(all_results)\n",
        "\n",
        "        else:\n",
        "            results, models = main_integrated_experiment()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExperiment interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during execution: {e}\")\n",
        "        # Fallback to synthetic data demo\n",
        "        print(\"\\nFalling back to synthetic data demo...\")\n",
        "        quick_demo()"
      ]
    }
  ]
}